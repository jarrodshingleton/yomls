---
title: "Week 10 In Class Lab"
author: "Jarrod Shingleton"
date: "5/25/2020"
output: md_document
---



<div id="random-forest-modeling-overview" class="section level1">
<h1>Random Forest Modeling Overview</h1>
<p>In this practical application exercise, you will:</p>
<ul>
<li>Review on fitting a classification tree for the lecture (Default) dataset using provided code</li>
<li>Fit a random forest model to the same dataset using provided code</li>
</ul>
</div>
<div id="step-1-set-up-r-environment-for-lecture-example-analysis" class="section level1">
<h1>Step 1: Set Up R Environment for Lecture Example Analysis</h1>
<p>Let’s begin by setting up our environment to continue with our lecture examples. First, make sure you have all the needed packages loaded and add some new ones for fitting Classification and Regression Trees (CART), Random Forest Models, and for making some pretty pictures (note - the standard way to do this in the R community is to always put the needed libraries at the very top of your script, so adjust your script file accordingly):</p>
<pre class="r"><code>### Load needed libraries
library(e1071)
library(ISLR)
library(ggplot2)
library(gridExtra)
library(caret)

# New model libraries
library(rpart) # new library for CART modeling
library(randomForest) # new library added for random forests and bagging

# New package for making pretty pictures of CART trees
library(rpart.plot)</code></pre>
</div>
<div id="step-2-confirm-datasets-loaded-and-visualize" class="section level1">
<h1>Step 2: Confirm Datasets Loaded and Visualize</h1>
<p>As a reminder of the data we are working with for the lecture examples, here is a quick summary and plot of the “Default” dataset containing all the data (this isn’t split into training and test datasets).</p>
<pre class="r"><code># Partition of data set into 80% Train and 20% Test datasets
set.seed(123)  # ensures we all get the sample sample of data for train/test

sampler &lt;- sample(nrow(Default),trunc(nrow(Default)*.80)) # samples index 

LectureTrain &lt;- Default[sampler,]
LectureTest &lt;- Default[-sampler,]

# Create a plotting version of the Default dataset where we will store model predictions
LectureTestPlotting &lt;- LectureTest</code></pre>
<pre class="r"><code># Testing data summary
summary(LectureTest)</code></pre>
<pre><code>##  default    student       balance           income     
##  No :1934   No :1393   Min.   :   0.0   Min.   : 5297  
##  Yes:  66   Yes: 607   1st Qu.: 481.0   1st Qu.:21146  
##                        Median : 814.3   Median :34510  
##                        Mean   : 831.9   Mean   :33527  
##                        3rd Qu.:1166.7   3rd Qu.:43919  
##                        Max.   :2461.5   Max.   :70022</code></pre>
<pre class="r"><code>## Plot Lecture Train Data
plotTrain &lt;- ggplot(data = LectureTrain,
                   mapping = aes(x = balance, y = income, color = default, shape = student)) +
  layer(geom = &quot;point&quot;, stat = &quot;identity&quot;, position = &quot;identity&quot;) +
  scale_color_manual(values = c(&quot;No&quot; = &quot;blue&quot;, &quot;Yes&quot; = &quot;red&quot;)) +
  theme_bw() +
  theme(legend.key = element_blank()) +
  labs(title = &quot;Lecture Training Data&quot;)

# Plot Lecture Test Data
plotTest &lt;- ggplot(data = LectureTest,
                    mapping = aes(x = balance, y = income, color = default, shape = student)) +
  layer(geom = &quot;point&quot;, stat = &quot;identity&quot;, position = &quot;identity&quot;) +
  scale_color_manual(values = c(&quot;No&quot; = &quot;blue&quot;, &quot;Yes&quot; = &quot;red&quot;)) +
  theme_bw() +
  theme(legend.key = element_blank()) +
  labs(title = &quot;Lecture Test Data&quot;)
grid.arrange(plotTrain, plotTest, nrow = 2)</code></pre>
<p><img src="/post/adaboost/ADABoost_files/figure-html/Plotting%20Default%20Data-1.png" width="672" /></p>
</div>
<div id="recall-cart-model" class="section level1">
<h1>Recall: CART model</h1>
<div id="step-3-build-a-classification-tree" class="section level2">
<h2>Step 3: Build a Classification Tree</h2>
<p>As discussed in the lecture, random forest models are an extended application of classification trees. Therefore, before fitting random forest models, let’s fit a classification tree to our data and see how we do (this will also allow us to see if extending to a random forest model improves our performance). The code block below fits a classification tree using the rpart package and then visualizes it using the prp() function from the rpart.plot package.</p>
<pre class="r"><code>CART &lt;- rpart(default ~., data=LectureTrain)
summary(CART)</code></pre>
<pre><code>## Call:
## rpart(formula = default ~ ., data = LectureTrain)
##   n= 8000 
## 
##           CP nsplit rel error    xerror       xstd
## 1 0.09925094      0 1.0000000 1.0000000 0.06016908
## 2 0.02247191      2 0.8014981 0.8314607 0.05502426
## 3 0.01000000      3 0.7790262 0.8389513 0.05526445
## 
## Variable importance
## balance  income student 
##      94       4       2 
## 
## Node number 1: 8000 observations,    complexity param=0.09925094
##   predicted class=No   expected loss=0.033375  P(node) =1
##     class counts:  7733   267
##    probabilities: 0.967 0.033 
##   left son=2 (7754 obs) right son=3 (246 obs)
##   Primary splits:
##       balance &lt; 1788.617 to the left,  improve=130.6222000, (0 missing)
##       student splits as  LR,           improve=  0.6965178, (0 missing)
##       income  &lt; 26430.88 to the right, improve=  0.6034235, (0 missing)
## 
## Node number 2: 7754 observations
##   predicted class=No   expected loss=0.0172814  P(node) =0.96925
##     class counts:  7620   134
##    probabilities: 0.983 0.017 
## 
## Node number 3: 246 observations,    complexity param=0.09925094
##   predicted class=Yes  expected loss=0.4593496  P(node) =0.03075
##     class counts:   113   133
##    probabilities: 0.459 0.541 
##   left son=6 (149 obs) right son=7 (97 obs)
##   Primary splits:
##       balance &lt; 1972.743 to the left,  improve=17.320740, (0 missing)
##       income  &lt; 27432.68 to the left,  improve= 8.238607, (0 missing)
##       student splits as  RL,           improve= 4.531466, (0 missing)
##   Surrogate splits:
##       income &lt; 49819.21 to the left,  agree=0.622, adj=0.041, (0 split)
## 
## Node number 6: 149 observations,    complexity param=0.02247191
##   predicted class=No   expected loss=0.3892617  P(node) =0.018625
##     class counts:    91    58
##    probabilities: 0.611 0.389 
##   left son=12 (93 obs) right son=13 (56 obs)
##   Primary splits:
##       income  &lt; 27874.34 to the left,  improve=4.844486, (0 missing)
##       student splits as  RL,           improve=2.448568, (0 missing)
##       balance &lt; 1856.76  to the left,  improve=1.547335, (0 missing)
##   Surrogate splits:
##       student splits as  RL,           agree=0.913, adj=0.768, (0 split)
##       balance &lt; 1962.872 to the left,  agree=0.638, adj=0.036, (0 split)
## 
## Node number 7: 97 observations
##   predicted class=Yes  expected loss=0.2268041  P(node) =0.012125
##     class counts:    22    75
##    probabilities: 0.227 0.773 
## 
## Node number 12: 93 observations
##   predicted class=No   expected loss=0.2903226  P(node) =0.011625
##     class counts:    66    27
##    probabilities: 0.710 0.290 
## 
## Node number 13: 56 observations
##   predicted class=Yes  expected loss=0.4464286  P(node) =0.007
##     class counts:    25    31
##    probabilities: 0.446 0.554</code></pre>
<pre class="r"><code># Make a plot of the classification tree rules
prp(CART)</code></pre>
<p><img src="/post/adaboost/ADABoost_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Look at that. You now have a visual summary of how the CART model (in this case a classification model) is going to make a decision about whether or not someone is going to default. CART models offer maximum explainability. We have found that one of the great benefits to using random forest models is that it is relatively easy to explain to people how a random forest model works if you show them a CART output first (hence this portion of the practical exercise). Let’s see how this model performs on the test dataset:</p>
<pre class="r"><code># Summarize and plot the performance of the classification tree

# Get the probability classes for CART model applied to test dataset
predClassCART &lt;- predict(CART, newdata = LectureTest, type = &quot;class&quot;)

# Add to our plotting dataframe
LectureTestPlotting$predClassCART &lt;- predClassCART

## Plot the CART class
plotClassCART &lt;- ggplot(data = LectureTestPlotting,
                       mapping = aes(x = balance, y = income, color = predClassCART, shape = student)) +
  layer(geom = &quot;point&quot;, stat = &quot;identity&quot;, position = &quot;identity&quot;) +
  scale_color_manual(values = c(&quot;No&quot; = &quot;blue&quot;, &quot;Yes&quot; = &quot;red&quot;)) +
  theme_bw() +
  theme(legend.key = element_blank()) +
  labs(title = &quot;Predicted class for CART Model&quot;)
grid.arrange(plotTest, plotClassCART, nrow = 2)</code></pre>
<p><img src="/post/adaboost/ADABoost_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code># Report confusion matrix from on the test dataset
confusionMatrix(predClassCART, LectureTest$default)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  1921   39
##        Yes   13   27
##                                          
##                Accuracy : 0.974          
##                  95% CI : (0.966, 0.9805)
##     No Information Rate : 0.967          
##     P-Value [Acc &gt; NIR] : 0.0417631      
##                                          
##                   Kappa : 0.4969         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.0005265      
##                                          
##             Sensitivity : 0.9933         
##             Specificity : 0.4091         
##          Pos Pred Value : 0.9801         
##          Neg Pred Value : 0.6750         
##              Prevalence : 0.9670         
##          Detection Rate : 0.9605         
##    Detection Prevalence : 0.9800         
##       Balanced Accuracy : 0.7012         
##                                          
##        &#39;Positive&#39; Class : No             
## </code></pre>
</div>
</div>
<div id="random-forest" class="section level1">
<h1>Random Forest</h1>
<div id="step-4-build-a-random-forest-model" class="section level2">
<h2>Step 4: Build a Random Forest Model</h2>
<p>Fitting a random forest model in R using the randomForest package is a simple extension of the syntax we’ve been using for all of the model fitting. As an aside, for each of these models, you can learn about the models by typing a question mark into the command line followed by the command you want to investigate. For example, type the following into your command line: ?randomForest.</p>
<p>You will see that in the lower right window of your RStudio instance, you get a report that tells you about all of the parameters you can adjust for your models. In this course, we are not going in depth into the tuning of the models we are fitting (this is just an overview course), but to become truly proficient in data science you should understand all of these parameters and how to adjust them appropriately.</p>
<p>For this example, we are going to accept the package defaults, get information about the predictor variable importance (importance = TRUE), and fit 500 trees (ntrees = 500).</p>
<pre class="r"><code># Applyrandom forests model having default target and rest as predictors
RandomForest &lt;- randomForest(default ~ ., data=LectureTrain, importance = TRUE, ntrees = 500)
summary(RandomForest)</code></pre>
<pre><code>##                 Length Class  Mode     
## call                5  -none- call     
## type                1  -none- character
## predicted        8000  factor numeric  
## err.rate         1500  -none- numeric  
## confusion           6  -none- numeric  
## votes           16000  matrix numeric  
## oob.times        8000  -none- numeric  
## classes             2  -none- character
## importance         12  -none- numeric  
## importanceSD        9  -none- numeric  
## localImportance     0  -none- NULL     
## proximity           0  -none- NULL     
## ntree               1  -none- numeric  
## mtry                1  -none- numeric  
## forest             14  -none- list     
## y                8000  factor numeric  
## test                0  -none- NULL     
## inbag               0  -none- NULL     
## terms               3  terms  call</code></pre>
<p>Now, let’s do the performance summaries on the test dataset that are becoming standard (visualization and confusion matrix statistics).</p>
<pre class="r"><code># Summarize and plot the performance of the random forest model

# Get the probability of &quot;yes&quot; for default from second column
predProbRF &lt;- predict(RandomForest, newdata = LectureTest, type = &quot;prob&quot;)[,2]

# Get the predicted class
predClassRF &lt;- predict(RandomForest, newdata = LectureTest, type = &quot;response&quot;)


# Add to our plotting dataframe
LectureTestPlotting$predProbRF &lt;- predProbRF
LectureTestPlotting$predClassRF &lt;- predClassRF

## Plot the RF Probability
plotProbRF &lt;- ggplot(data = LectureTestPlotting,
                      mapping = aes(x = balance, y = income, color = predProbRF, shape = student)) +
  layer(geom = &quot;point&quot;, stat = &quot;identity&quot;, position = &quot;identity&quot;) +
  scale_color_gradient(low = &quot;blue&quot;, high = &quot;red&quot;) +
  theme_bw() +
  theme(legend.key = element_blank()) +
  labs(title = &quot;Predicted Probability for RF Model&quot;)

# Plot the RF Class
plotClassRF &lt;- ggplot(data = LectureTestPlotting,
                        mapping = aes(x = balance, y = income, color = predClassRF, shape = student)) +
  layer(geom = &quot;point&quot;, stat = &quot;identity&quot;, position = &quot;identity&quot;) +
  scale_color_manual(values = c(&quot;No&quot; = &quot;blue&quot;, &quot;Yes&quot; = &quot;red&quot;)) +
  theme_bw() +
  theme(legend.key = element_blank()) +
  labs(title = &quot;Predicted Class for RF Model&quot;)

# Standard Performance Plot
grid.arrange(plotTest, plotProbRF, plotClassRF, nrow = 3)</code></pre>
<p><img src="/post/adaboost/ADABoost_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code># Report confusion matrix from on the test dataset
confusionMatrix(predClassRF , LectureTest$default)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  1929   48
##        Yes    5   18
##                                           
##                Accuracy : 0.9735          
##                  95% CI : (0.9655, 0.9801)
##     No Information Rate : 0.967           
##     P-Value [Acc &gt; NIR] : 0.0552          
##                                           
##                   Kappa : 0.3942          
##                                           
##  Mcnemar&#39;s Test P-Value : 7.968e-09       
##                                           
##             Sensitivity : 0.9974          
##             Specificity : 0.2727          
##          Pos Pred Value : 0.9757          
##          Neg Pred Value : 0.7826          
##              Prevalence : 0.9670          
##          Detection Rate : 0.9645          
##    Detection Prevalence : 0.9885          
##       Balanced Accuracy : 0.6351          
##                                           
##        &#39;Positive&#39; Class : No              
## </code></pre>
<p>Save your stuff!</p>
</div>
</div>
